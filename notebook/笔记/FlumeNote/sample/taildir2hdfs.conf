# Name the components on this agent
# a1: 当前agent的名称
# 当同时启动多个agent的时候,名称不能相同

# 数据源
a1.sources = r1
# 输出
a1.sinks = k1
# 通道
a1.channels = c1

a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/flume-1.9.0/job/taildir_position.json
a1.sources.r1.filegroups = flume hive
# 文件名要使用正则
a1.sources.r1.filegroups.flume = /opt/flume-1.9.0/logs/.*
a1.sources.r1.filegroups.hive = /opt/hive-3.1.2/logs/.*
a1.sources.r1.headers.flume.headerKey1 = flume
a1.sources.r1.headers.hive.headerKey1 = hive
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hd01:8020/flume/events/%y-%m-%d/%H%M/%S
a1.sinks.k1.hdfs.filePrefix = events-
# 设置文件滚动
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 1
a1.sinks.k1.hdfs.roundUnit = hour
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.fileType = DataStream
# 以下三个条件任意满足则生成新文件
# 每经过指定秒数生成一个新文件
a1.sinks.k1.hdfs.rollInterval = 3600
# 设置文件最大大小(略小于128M)
a1.sinks.k1.hdfs.rollSize = 134217700
# 按照事件量生成一个新文件(0: 代表不生成)
a1.sinks.k1.hdfs.rollCount = 0
# Use a channel which buffers events in memory
# 配置通道
# 通道类型
a1.channels.c1.type = memory
# 通道容量
a1.channels.c1.capacity = 1000
# 单个事务最大容量
a1.channels.c1.transactionCapacity = 100


# 绑定
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1